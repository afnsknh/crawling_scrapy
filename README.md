# Web Crawling Menggunakan Scrapy Framework 

## Definisi

​	*Web Crawler* merupakan sebuah program atau script yang menggunakan metode tertentu yang memiliki tujuan untuk mengumpulkan secara otomatis semua data/informasi yang berada pada suatu website. 

​	Proses dari *web crawler* mengunjungi tiap dokumen dari website ini lah yang disebut dengan *Web Crawling*.

## Fungsi

​	Tujuan utama dari *web crawler* adalah untuk mengumpulkan informasi-informasi yang ada dalam suatu halaman web.

​	Contoh penggunaannya yang paling umum adalah *search engine* yang memiliki tujuan untuk menampilkan data/informasi yang relevan dengan permintaan atau *keyword* yang dicari oleh *user*.

## Tentang Scrapy



## Web Crawling dengan Scrapy

1. Install Python

   - Python 2.7
   - Python 3 

2. Install Scrapy Menggunakan Pip

3. Scrapy Shell

4. Membuat Project Baru Menggunakan Scrapy

5. Membuat File Spider Baru

6. Edit File Spider

7. Export Data Crawling Menjadi CSV, JSON, atau Excel

   

## Kesimpulan

```python

```

